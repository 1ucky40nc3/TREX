{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TREX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN44fQuSm5BeqCIfhYrG9G9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1ucky40nc3/TREX/blob/main/TREX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sCNgpK3tZbZ",
        "outputId": "67e64bf1-ff3f-4b15-ece2-aca7477832bf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 26 16:58:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78i3mCfXdyK1",
        "cellView": "form"
      },
      "source": [
        "# @title Utils for the entire Notebook\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "from IPython.utils.io import capture_output\n",
        "\n",
        "\n",
        "def execute(func, *args, verbose: bool = False, **kwargs):\n",
        "    if verbose:\n",
        "        return func(*args, **{\"verbose\": verbose, **kwargs})\n",
        "    \n",
        "    with capture_output() as captured:\n",
        "        return func(*args, **{\"verbose\": verbose, **kwargs})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0AFeWgzwFgr"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Natural Language Processing (NLP)*** üì∞ü§Ø\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC4zn53LYTw9",
        "cellView": "form"
      },
      "source": [
        "# @title | NLP | Install Dependencies ‚á©\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "    \n",
        "\n",
        "def install_nlp_dependencies(**kwargs):\n",
        "    !pip install sentencepiece\n",
        "    !pip install transformers\n",
        "    !pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "    !pip install torch-geometric\n",
        "    !pip install torch-scatter==2.0.8 -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "\n",
        "\n",
        "execute(install_nlp_dependencies, verbose=VERBOSE)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1U9U9gdwKig",
        "cellView": "form"
      },
      "source": [
        "# @title | NLP | Initialize the NLP Pipelines\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "\n",
        "def device(boolean: bool) -> int:\n",
        "    return 0 if boolean else -1\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Model selection for the NLP toolkit ü§ñüì∞\n",
        "ZERO_SHOT_MODEL = \"facebook/bart-large-mnli\" #@param [\"facebook/bart-large-mnli\", \"typeform/distilbert-base-uncased-mnli\", \"joeddav/xlm-roberta-large-xnli\", \"Narsil/deberta-large-mnli-zero-cls\"]\n",
        "TABLE_QA_MODEL = \"google/tapas-large-finetuned-wikisql-supervised\" #@param [\"lysandre/tiny-tapas-random-wtq\", \"lysandre/tiny-tapas-random-sqa\", \"google/tapas-base-finetuned-wtq\", \"google/tapas-base-finetuned-sqa\", \"google/tapas-base-finetuned-wikisql-supervised\", \"google/tapas-large-finetuned-wtq\", \"google/tapas-large-finetuned-sqa\", \"google/tapas-large-finetuned-wikisql-supervised\"]\n",
        "SMALL_TALK_MODEL = \"facebook/blenderbot-90M\" #@param [\"facebook/blenderbot-90M\", \"facebook/blenderbot-400M-distill\", \"facebook/blenderbot-1B-distill\", \"facebook/blenderbot-3B\"]\n",
        "FEW_SHOT_MODEL = \"EleutherAI/gpt-neo-125M\" #@param [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"EleutherAI/gpt-neo-125M\", \"EleutherAI/gpt-neo-1.3B\", \"EleutherAI/gpt-neo-2.7B\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Language selection during operation üè≥Ô∏è‚Äçüåà/üè¥‚Äç‚ò†Ô∏è\n",
        "LANGUAGE = \"de\" #@param [\"en\", \"de\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Model selection for translation between English and German\n",
        "GERMAN_TO_ENGLISH_MODEL = \"facebook/wmt19-de-en\" #@param [\"Helsinki-NLP/opus-mt-de-en\", \"facebook/wmt19-de-en\"]\n",
        "ENGLISH_TO_GERMAN_MODEL = \"facebook/wmt19-en-de\" #@param [\"Helsinki-NLP/opus-mt-en-de\", \"facebook/wmt19-en-de\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Select if the individual model shall be on GPU üíªüî•\n",
        "USE_GPU_FOR_ZERO_SHOT = False # @param {type:\"boolean\"}\n",
        "USE_GPU_FOR_SMALL_TALK = True # @param {type:\"boolean\"}\n",
        "USE_GPU_FOR_FEW_SHOT = False # @param {type:\"boolean\"}\n",
        "\n",
        "USE_GPU_FOR_GERMAN_TO_ENGLISH = False # @param {type:\"boolean\"}\n",
        "USE_GPU_FOR_ENGLISH_TO_GERMAN = False # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = True # @param {type:\"boolean\"}\n",
        "\n",
        "def initialize_nlp_pipelines(**kwargs):\n",
        "    print(\"[DEBUG] Downloading Zero-Shot-Classification Components\")\n",
        "    ZERO_SHOT = transformers.pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=ZERO_SHOT_MODEL,\n",
        "        device=device(USE_GPU_FOR_ZERO_SHOT))\n",
        "    \n",
        "    print(\"[DEBUG] Downloading Table QA Components\")\n",
        "    TABLE_QA = transformers.pipeline(\n",
        "        \"table-question-answering\", \n",
        "        model=TABLE_QA_MODEL)\n",
        "\n",
        "    print(\"[DEBUG] Downloading Small-Talk Components\")\n",
        "    SMALL_TALK = transformers.pipeline(\n",
        "        \"conversational\", \n",
        "        model=SMALL_TALK_MODEL, \n",
        "        device=device(USE_GPU_FOR_SMALL_TALK))\n",
        "\n",
        "    print(\"[DEBUG] Downloading Text-To-Text Components\")\n",
        "    FEW_SHOT = transformers.pipeline(\n",
        "        \"text-generation\", \n",
        "        model=FEW_SHOT_MODEL, \n",
        "        device=device(USE_GPU_FOR_FEW_SHOT))\n",
        "    \n",
        "    if LANGUAGE == \"de\":\n",
        "        print(\"[DEBUG] Downloading German-To-English Translation Components\")\n",
        "        GERMAN_TO_ENGLISH_TRANSLATOR = transformers.pipeline(\n",
        "            \"translation_de_to_en\", \n",
        "            model=GERMAN_TO_ENGLISH_MODEL)\n",
        "        print(\"[DEBUG] Downloading English-To-German Translation Components\")\n",
        "        ENGLISH_TO_GERMAN_TRANSLATOR = transformers.pipeline(\n",
        "            \"translation_en_to_de\", \n",
        "            model=ENGLISH_TO_GERMAN_MODEL)\n",
        "    \n",
        "    return locals()\n",
        "\n",
        "PIPELINES = execute(initialize_nlp_pipelines, verbose=VERBOSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "N4aTN97_w5sH",
        "cellView": "form",
        "outputId": "c895fa40-fd22-4825-b53f-4b48d452809a"
      },
      "source": [
        "# @title | NLP | Set up Services for Data\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "import transformers\n",
        "\n",
        "\n",
        "TRAIN_TABLE = \"\"\"Location,Train,Start,Destination,Departure,Arrival,Delay\n",
        "Munich,ICE 77,Munich,Berlin,12:30,13:33,2\n",
        "Munich,ICE 56,Munich,Leipzig,12:30,13:30,4\n",
        "Munich,ICE 33,Leipzig,Berlin,13:30,14:00,45\"\"\"\n",
        "\n",
        "PLANE_TABLE = \"\"\"Location,Plane,Start,Destination,Departure,Arrival,Delay\n",
        "Munich,Eurowings (EW8003),Munich,Berlin,12:30,13:33,0\n",
        "Munich,Lufthansa (LH1940),Munich,Leipzig,12:30,13:30,10\n",
        "Munich,Lufthansa (LH2040),Leipzig,Berlin,13:30,14:00,7\"\"\"\n",
        "\n",
        "\n",
        "def train_table() -> pd.DataFrame:\n",
        "    return pd.read_csv(io.StringIO(TRAIN_TABLE))\n",
        "\n",
        "def plane_table() -> pd.DataFrame:\n",
        "    return pd.read_csv(io.StringIO(PLANE_TABLE))\n",
        "\n",
        "def df_to_csv(df) -> str:\n",
        "    csv = io.StringIO()\n",
        "    df.to_csv(csv, index=False)\n",
        "    return csv.getvalue()\n",
        "\n",
        "def csv_to_df(csv) -> pd.DataFrame:\n",
        "    return pd.read_csv(io.StringIO(csv))\n",
        "\n",
        "def travel_tables(tables=[train_table(), plane_table()]) -> pd.DataFrame:\n",
        "    travel = pd.concat(tables)\n",
        "    travel = travel.fillna(\"NONE\")\n",
        "\n",
        "    csv = df_to_csv(travel)\n",
        "    travel = csv_to_df(csv)\n",
        "    travel = travel.astype(\n",
        "        {column: str for column in travel.columns.values})\n",
        "    return travel\n",
        "\n",
        "travel_samples = \"\"\"Question: \"I am in Hannover. It is 17:48. Which train can I take from Munich to Berlin?\" Context: \"ICE 33\" Answer: \"The train ICE 33 will travel from Munich to Berlin.\"\n",
        "Question: \"I am in Frankfurt. It is 09:32. When will the next flight to Madrid leave?\" Context: \"07:59\" Answer: \"The next to Madrid will leave 07:59.\"\n",
        "Question: \"I am in Hannover. It is 17:48. What is the best train to get to Berlin from London?\" Context: \"ICE 33\" Answer: \"The train ICE 33 is the fastest.\"\n",
        "Question: \"I am in Munich. It is 05:51. From which platform does the RB 61 exit to Zurich?\" Context: \"8\" Answer: \"The RB 61 will departs from the platform 8.\"\n",
        "Question: \"I am in Berlin. It is 12:20. How can I get to London by aircraft?\" Context: \"Eurowings (EW8003)\" Answer: \"The flight Eurowings (EW8003) will leave for London.\"\n",
        "Question: \"I am in London. It is 12:20. I am at a train station. How can I get to Frankfurt?\" Context: \"ICE 923\" Answer: \"The train ICE 77 is available for your trip.\"\n",
        "Question: \"I am in Hamburg. It is 23:43. From which gate does the Lufthansa aircraft (LH1940) take off?\" Context: \"23\" Answer: \"The Lufthansa airliner (LH1940) will take off from gate 23.\"\n",
        "Question: \"I am in Stuttgart. It is 08:24. When will the next ICE to Bremen leave?\" Context: \"07:59\" Answer: \"The next to Bremen will leave 07:59.\"\n",
        "Question: \"I am in Bremen. It is 20:15. Is the ICE 1556 delayed?\" Context: \"9\" Answer: \"The ICE 1556 will be delayed by 9 minutes.\"\n",
        "Question: \"I am in Dresden. It is 10:17. How much is the flight LH1239 delayed?\" Context: \"59\" Answer: \"The flight LH1239 will be delayed by 59 minutes.\"\n",
        "\"\"\"\n",
        "\n",
        "def samples_length(samples: str, model: str) -> int:\n",
        "    if not \"EleutherAI/gpt-neo\" in model:\n",
        "        raise Exception(\n",
        "            \"\"\"The tokenizer of the FEW-SHOT model is not accessible!\n",
        "            Therefore the sample length can not be computed.\"\"\")\n",
        "        \n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(FEW_SHOT_MODEL)\n",
        "    input_ids = tokenizer(travel_samples, return_tensors=\"pt\").input_ids\n",
        "    return input_ids.shape[-1]\n",
        "\n",
        "\n",
        "TRAVEL_TIME = \"7:00\" #@param {type: \"string\"}\n",
        "TRAVEL_LOCATION = \"Munich\" #@param {type: \"string\"}\n",
        "\n",
        "def travel_time() -> str:\n",
        "    return TRAVEL_TIME\n",
        "\n",
        "def travel_location() -> str:\n",
        "    return TRAVEL_LOCATION\n",
        "\n",
        "travel_tables()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Location</th>\n",
              "      <th>Train</th>\n",
              "      <th>Start</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Departure</th>\n",
              "      <th>Arrival</th>\n",
              "      <th>Delay</th>\n",
              "      <th>Plane</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Munich</td>\n",
              "      <td>ICE 77</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:33</td>\n",
              "      <td>2</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Munich</td>\n",
              "      <td>ICE 56</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:30</td>\n",
              "      <td>4</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Munich</td>\n",
              "      <td>ICE 33</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>13:30</td>\n",
              "      <td>14:00</td>\n",
              "      <td>45</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Munich</td>\n",
              "      <td>NONE</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:33</td>\n",
              "      <td>0</td>\n",
              "      <td>Eurowings (EW8003)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Munich</td>\n",
              "      <td>NONE</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:30</td>\n",
              "      <td>10</td>\n",
              "      <td>Lufthansa (LH1940)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Munich</td>\n",
              "      <td>NONE</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>13:30</td>\n",
              "      <td>14:00</td>\n",
              "      <td>7</td>\n",
              "      <td>Lufthansa (LH2040)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Location   Train    Start  ... Arrival Delay               Plane\n",
              "0   Munich  ICE 77   Munich  ...   13:33     2                NONE\n",
              "1   Munich  ICE 56   Munich  ...   13:30     4                NONE\n",
              "2   Munich  ICE 33  Leipzig  ...   14:00    45                NONE\n",
              "3   Munich    NONE   Munich  ...   13:33     0  Eurowings (EW8003)\n",
              "4   Munich    NONE   Munich  ...   13:30    10  Lufthansa (LH1940)\n",
              "5   Munich    NONE  Leipzig  ...   14:00     7  Lufthansa (LH2040)\n",
              "\n",
              "[6 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDT5V9m2w3sQ",
        "cellView": "form"
      },
      "source": [
        "# @title | NLP | NLP Implementation\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Tuple\n",
        "from typing import Callable\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "class Associations(Dict):\n",
        "    data: Callable[[], pd.DataFrame]\n",
        "    samples: str\n",
        "\n",
        "Response = Tuple[str, Optional[transformers.Conversation]]\n",
        "\n",
        "Function = Callable[\n",
        "    [transformers.Conversation], \n",
        "    Response\n",
        "]\n",
        "\n",
        "class Skill(Dict):\n",
        "    associations: Associations\n",
        "    function: Function\n",
        "\n",
        "class Skills(Dict):\n",
        "    name: str\n",
        "    skill: Skill\n",
        "\n",
        "WarmUp = Callable[\n",
        "    [transformers.Conversation], \n",
        "    transformers.Conversation\n",
        "]\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Classification on a Zero-Shot basis.\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def zero_shot_classification(input: str, \n",
        "                             labels: List[str], \n",
        "                             top_k: Optional[int] = 1,\n",
        "                             **kwargs) -> List[str]:\n",
        "    return PIPELINES[\"ZERO_SHOT\"](input, labels)[\"labels\"][:top_k]\n",
        "\n",
        "def skill_classification(input: str, \n",
        "                         skills: List[str], \n",
        "                         verbose: Optional[bool],\n",
        "                         **kwargs) -> List[str]:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Skill Classification| input: {input}\")\n",
        "        print(f\"[DEBUG] |Skill Classification| skills: {skills}\")\n",
        "\n",
        "    skill = zero_shot_classification(input, skills, **kwargs)[0]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Skill Classification| skill: {skill}\")\n",
        "    return skill\n",
        "\n",
        "def toxicity_classification(input: str,\n",
        "                            verbose: Optional[bool],\n",
        "                            labels: List[str] = [\"toxic\", \"non-toxic\"],\n",
        "                            **kwargs) -> List[str]:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Toxicity Classification| input: {input}\")\n",
        "        print(f\"[DEBUG] |Toxicity Classification| labels: {labels}\")\n",
        "\n",
        "    label = zero_shot_classification(input, labels, **kwargs)[0]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Toxicity Classification| label: {label}\")\n",
        "    return label\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~\n",
        "Section for Table QA.\n",
        "~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def table_question_answering(input: str, \n",
        "                             table: pd.DataFrame, \n",
        "                             verbose: Optional[bool] = False, \n",
        "                             **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Table Question Answering| input: {input}\")\n",
        "        print(f\"[DEBUG] |Table Question Answering| table: \\n{table}\")\n",
        "    \n",
        "    print(type(input), type(table))\n",
        "    output = PIPELINES[\"TABLE_QA\"](table=table, query=input)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Table Question Answering| output: \\n{output}\")\n",
        "    return output[\"answer\"]\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Few-Shot Text Generation\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def few_shot(query: str, \n",
        "             samples: str, \n",
        "             verbose: Optional[bool] = False, \n",
        "             **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Few-Shot Text Generation| query: \\n{query}\")\n",
        "        print(f\"[DEBUG] |Few-Shot Text Generation| samples: \\n{samples}\")\n",
        "    \n",
        "    output = PIPELINES[\"FEW_SHOT\"](samples + query, **kwargs)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Few-Shot Text Generation| output: \\n{output}\")\n",
        "    return output[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "def travel_few_shot(query: str, \n",
        "                    samples: str, \n",
        "                    verbose: Optional[bool] = False, \n",
        "                    **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Few-Shot Text Generation| query: \\n{query}\")\n",
        "        print(f\"[DEBUG] |Travel Few-Shot Text Generation| samples: \\n{samples}\")\n",
        "\n",
        "    output = few_shot(query, \n",
        "                      samples, \n",
        "                      verbose, \n",
        "                      **kwargs)\n",
        "    output = output[len(samples + query):]\n",
        "    output = output.split('\"')[0]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Few-Shot Text Generation| output: \\n{output}\")\n",
        "    return output\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Skill Functions\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def travel_skill(conversation: transformers.Conversation, \n",
        "                 associations: Associations, \n",
        "                 verbose: Optional[bool] = False, \n",
        "                 **kwargs) -> Response:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Skill| input conversation: \\n{conversation}\")\n",
        "        print(f\"[DEBUG] |Travel Skill| associations: \\n{associations}\")\n",
        "\n",
        "    input = conversation.new_user_input\n",
        "    labels = list(associations.keys())\n",
        "\n",
        "    variant = zero_shot_classification(input, labels, **kwargs)[0]\n",
        "\n",
        "    data = associations[variant][\"data\"]()\n",
        "    time = associations[variant][\"time\"]()\n",
        "    location = associations[variant][\"location\"]()\n",
        "    samples = associations[variant][\"samples\"]\n",
        "    config = associations[variant][\"config\"]\n",
        "\n",
        "    cell = table_question_answering(input, data, verbose)\n",
        "\n",
        "    input = f\"I am in {location}. It is {time}. {input}\"\n",
        "    query = f'Question: \"{input}\" Context: \"{cell}\" Answer: \"'\n",
        "\n",
        "    output = travel_few_shot(\n",
        "        query, \n",
        "        samples, \n",
        "        verbose, \n",
        "        **{**config, **kwargs})\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Skill| variant: {variant}\")\n",
        "        print(f\"[DEBUG] |Travel Skill| cell: {cell}\")\n",
        "        print(f\"[DEBUG] |Travel Skill| output: {output}\")\n",
        "    return output, None\n",
        "\n",
        "def small_talk_skill(conversation: transformers.Conversation, \n",
        "                     verbose: Optional[bool] = False, \n",
        "                     **kwargs) -> Response:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Small Talk Skill| input conversation: \\n{conversation}\")\n",
        "\n",
        "    conversation = PIPELINES[\"SMALL_TALK\"](conversation)\n",
        "    output = conversation.generated_responses[-1]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Small Talk Skill| output conversation: \\n{conversation}\")\n",
        "        print(f\"[DEBUG] |Small Talk Skill| output: {output}\")\n",
        "    return output, conversation\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Personas and Warm Up\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "\n",
        "def warm_up(conversation: transformers.Conversation, \n",
        "            personas: List[str],\n",
        "            verbose: bool = False,\n",
        "            **kwargs) -> transformers.Conversation:\n",
        "    for persona in personas:\n",
        "        conversation.add_user_input(persona)\n",
        "        conversation = PIPELINES[\"SMALL_TALK\"](conversation)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Warm Up| personas: {personas}\")\n",
        "        print(f\"[DEBUG] |Warm Up| personas: {conversation}\")\n",
        "    return conversation\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~\n",
        "Language Processors\n",
        "~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def german_to_english_translation(input: str,\n",
        "                                  verbose: Optional[bool] = False, \n",
        "                                  **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |German-To-English Translation| input: {input}\")\n",
        "\n",
        "    translation = PIPELINES[\"GERMAN_TO_ENGLISH_TRANSLATOR\"](input, **{\"num_beams\": 40, **kwargs})\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |German-To-English Translation| translation: {translation}\")\n",
        "    return translation[0][\"translation_text\"]\n",
        "\n",
        "def english_to_german_translation(input: str,\n",
        "                                  verbose: Optional[bool] = False, \n",
        "                                  **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |English-To-German Translation| input: {input}\")\n",
        "\n",
        "    translation = PIPELINES[\"ENGLISH_TO_GERMAN_TRANSLATOR\"](input, **{\"num_beams\": 40, **kwargs})\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |English-To-German Translation| translation: {translation}\")\n",
        "    return translation[0][\"translation_text\"]\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~\n",
        "NLP Configuration\n",
        "~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "LANGUAGES = {\n",
        "    \"en\": {\n",
        "        \"preprocessor\": lambda x: x,\n",
        "        \"postprocessor\": lambda x: x,\n",
        "    },\n",
        "    \"de\": {\n",
        "        \"preprocessor\": german_to_english_translation,\n",
        "        \"postprocessor\": english_to_german_translation,\n",
        "    }\n",
        "}\n",
        "\n",
        "TRAVEL_ASSOCIATIONS = {\n",
        "    \"travel\": {\n",
        "        \"data\": travel_tables,\n",
        "        \"time\": travel_time,\n",
        "        \"location\": travel_location,\n",
        "        \"samples\": travel_samples,\n",
        "        \"config\": {\n",
        "            \"temperature\": 0.6,\n",
        "            \"do_sample\": True,\n",
        "            \"max_length\": samples_length(\n",
        "                travel_samples, \n",
        "                FEW_SHOT_MODEL) + 50,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "TRAVEL_SKILL = {\n",
        "    \"associations\": TRAVEL_ASSOCIATIONS, \n",
        "    \"function\": travel_skill\n",
        "}\n",
        "\n",
        "SKILLS: Skills = {\n",
        "    \"small talk\": {\n",
        "        \"associations\": {}, \n",
        "        \"function\": small_talk_skill\n",
        "    },\n",
        "    \"travel\": TRAVEL_SKILL,\n",
        "    \"travel on time\": TRAVEL_SKILL,\n",
        "    \"travel delayed\": TRAVEL_SKILL,\n",
        "    \"other\": {\n",
        "        \"associations\": {}, \n",
        "        \"function\": small_talk_skill\n",
        "    },\n",
        "}\n",
        "\n",
        "PERSONAS = []\n",
        "\n",
        "\n",
        "CONFIG = {\n",
        "    \"languages\": LANGUAGES,\n",
        "    \"personas\": PERSONAS,\n",
        "    \"skills\": SKILLS,\n",
        "} \n",
        "\n",
        "\n",
        "class NLP:\n",
        "    def __init__(self,\n",
        "                 config: dict = CONFIG,\n",
        "                 verbose: bool = True,\n",
        "                 **kwargs):\n",
        "        self.config = config\n",
        "\n",
        "        language_processors = config[\"languages\"][LANGUAGE]\n",
        "        self.language_preprocessor = language_processors[\"preprocessor\"]\n",
        "        self.language_postprocessor = language_processors[\"postprocessor\"]\n",
        "\n",
        "        self.conversation = transformers.Conversation()\n",
        "        self.conversation = warm_up(\n",
        "            self.conversation, \n",
        "            personas=config[\"personas\"],\n",
        "            verbose=verbose,\n",
        "            **kwargs)\n",
        "\n",
        "        self.skills = config[\"skills\"]\n",
        "        \n",
        "    def __call__(self, \n",
        "                 input: str, \n",
        "                 verbose: bool = False, \n",
        "                 **kwargs) -> str:\n",
        "        if verbose:\n",
        "            print(f\"[DEBUG] |NLP __call__ START|\" + \"~\"*20)\n",
        "            print(f\"[DEBUG] |NLP ATTR skills|: {self.skills}\")\n",
        "            print(f\"[DEBUG] |NLP ATTR conversation|: \\n{self.conversation}\")\n",
        "            print(f\"[DEBUG] |NLP User input|: {input}\")\n",
        "        \n",
        "        input = self.language_preprocessor(input)\n",
        "        self.conversation.add_user_input(input)\n",
        "\n",
        "        skill = skill_classification(\n",
        "            input, \n",
        "            list(self.skills.keys()), \n",
        "            verbose=verbose, \n",
        "            **kwargs)\n",
        "        \n",
        "        pipeline = self.skills[skill]\n",
        "        function = pipeline[\"function\"]\n",
        "        associations = pipeline[\"associations\"]\n",
        "        \n",
        "        output, conversation = function(\n",
        "            self.conversation, \n",
        "            associations=associations, \n",
        "            verbose=verbose,\n",
        "            **kwargs)\n",
        "\n",
        "        if conversation:\n",
        "            self.conversation = conversation\n",
        "        output = self.language_postprocessor(output)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[DEBUG] |NLP conversation |: \\n{self.conversation}\")\n",
        "            print(f\"[DEBUG] |NLP User output|: {output}\")\n",
        "            print(f\"[DEBUG] |NLP __call__ END|\" + \"~\"*20)\n",
        "        return output\n",
        "\n",
        "nlp = execute(NLP, verbose=VERBOSE, config=CONFIG)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Zo09hxv31Y"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Speech Recognition (STT)*** üé§üí¨\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7WGmmWMbf4o",
        "cellView": "form"
      },
      "source": [
        "# @title | STT | Installation of Dependencies ‚á©\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "\n",
        "def install_sst_dependencies(**kwargs):\n",
        "    !pip install transformers\n",
        "    !pip install numpy==1.20\n",
        "    !pip install numba==0.48\n",
        "    !pip install ffmpeg-python\n",
        "    !pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n",
        "\n",
        "execute(install_sst_dependencies, verbose=VERBOSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PauhmEq8chkm",
        "cellView": "form"
      },
      "source": [
        "# @title | STT | Audio Recording Utils\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\"\"\"Utils for recording audio in a Google Colaboratory notebook.\n",
        "\n",
        "This code is adapted from:\n",
        "    https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/\n",
        "    https://colab.research.google.com/gist/ricardodeazambuja/03ac98c31e87caf284f7b06286ebf7fd/microphone-to-numpy-array-from-your-browser-in-colab.ipynb\n",
        "\"\"\"\n",
        "\n",
        "SILENT = \"&> /dev/null\"\n",
        "\n",
        "import io\n",
        "import ffmpeg\n",
        "import numpy as np\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import write\n",
        "from dl_colab_notebooks.audio import audio_bytes_to_np\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import HTML\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "\n",
        "STYLES_HTML = \"\"\"\n",
        "<script>\n",
        "\n",
        "var styles = `\n",
        "\n",
        "button {\n",
        "    width: 300px;\n",
        "    height: 54px;\n",
        "\n",
        "    padding: 20px;\n",
        "    margin: 5px;\n",
        "\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    align-items: center;\n",
        "    border-radius: 40px;\n",
        "    border: none;\n",
        "\n",
        "    text-align: center;\n",
        "    font-size: 28px;\n",
        "    \n",
        "    transition: all 0.5s;\n",
        "    cursor: pointer;\n",
        "}\n",
        "\n",
        "button span {\n",
        "    display: inline-block;\n",
        "    position: relative;\n",
        "\n",
        "    cursor: pointer;\n",
        "    transition: 0.5s;\n",
        "}\n",
        "\n",
        "button span:after {\n",
        "    content: 'üôè';\n",
        "\n",
        "    position: absolute;\n",
        "    right: -20px;\n",
        "\n",
        "    opacity: 0;\n",
        "    transition: 0.5s;\n",
        "}\n",
        "\n",
        "button:hover span {\n",
        "    padding-right: 25px;\n",
        "}\n",
        "\n",
        "button:hover span:after {\n",
        "    right: 0;\n",
        "    opacity: 1;\n",
        "}\n",
        "`\n",
        "\n",
        "var styleSheet = document.createElement(\"style\")\n",
        "styleSheet.type = \"text/css\"\n",
        "styleSheet.innerText = styles\n",
        "document.head.appendChild(styleSheet);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "\n",
        "var container = document.createElement(\"div\");\n",
        "var button = document.createElement(\"button\");\n",
        "var span = document.createElement(\"span\");\n",
        "\n",
        "button.appendChild(span);\n",
        "container.appendChild(button);\n",
        "document.body.appendChild(container);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader, recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "            mimeType : 'audio/webm;codecs=opus'\n",
        "    };            \n",
        "    recorder = new MediaRecorder(stream);\n",
        "    recorder.ondataavailable = function(e) {            \n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        container.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data); \n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "span.innerText = \"‚è∏Ô∏é\";\n",
        "button.style.verticalAlign = \"middle\";\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "function toggleRecording() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        span.innerText = \"‚úÖ\"\n",
        "    }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "    return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve => {\n",
        "    button.onclick = () => {\n",
        "        toggleRecording()\n",
        "\n",
        "        sleep(2000).then(() => {\n",
        "            resolve(base64data.toString())\n",
        "        });\n",
        "    }\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def record(sample_rate: int = 16000) -> str:\n",
        "    display(HTML(STYLES_HTML + AUDIO_HTML))\n",
        "    data = eval_js(\"data\")\n",
        "    \n",
        "    audio_bytes = b64decode(data.split(',')[1])\n",
        "    return audio_bytes_to_np(audio_bytes, sample_rate)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BjsRML5VtFXY"
      },
      "source": [
        "# @title | STT | Wav2Vec2 Speech Recognition\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def speech_to_text_implementation(**kwargs):\n",
        "    from transformers import Wav2Vec2Tokenizer\n",
        "    from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "    STT_MODEL = \"facebook/wav2vec2-large-960h-lv60-self\" if LANGUAGE == \"en\" else \"facebook/wav2vec2-large-xlsr-53-german\"\n",
        "\n",
        "    # load model and tokenizer\n",
        "    tokenizer = Wav2Vec2Tokenizer.from_pretrained(STT_MODEL)\n",
        "    wav2vec2 = Wav2Vec2ForCTC.from_pretrained(STT_MODEL)\n",
        "\n",
        "    def speech_to_text(audio: np.ndarray) -> List[str]:   \n",
        "        input_values = tokenizer(\n",
        "            [audio], \n",
        "            return_tensors=\"pt\", \n",
        "            padding=\"longest\"\n",
        "        ).input_values\n",
        "\n",
        "        logits = wav2vec2(input_values).logits\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        text = tokenizer.batch_decode(predicted_ids)\n",
        "        text = \" \".join(text)\n",
        "        return text\n",
        "\n",
        "    return speech_to_text\n",
        "\n",
        "speech_to_text = execute(speech_to_text_implementation, verbose=VERBOSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXF876tBJ6I8"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Text-To-Speech (TTS)*** üí≠üì£\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5UxGOjsiJ6JF"
      },
      "source": [
        "# @title | TTS | Installation of Dependencies ‚á©\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "\n",
        "def install_tts_dependencies(**kwargs):\n",
        "    !apt-get install -y espeak\n",
        "\n",
        "    if LANGUAGE == \"de\":\n",
        "        !gdown --id 1VG0EI7J6S1bk3h0q1VBc9ALExkdZdeVm -O tts_model.pth.tar\n",
        "        !gdown --id 1s1GcSihlj58KX0LeA-FPFvdMWGMkcxKI -O config.json\n",
        "        !gdown --id 1zYFHElvYW_oTeilvbZVLMLscColWRbck -O vocoder_model.pth.tar\n",
        "        !gdown --id 1ye9kVDbatAKMncRMui7watrLQ_5DaJ3e -O config_vocoder.json\n",
        "        !gdown --id 1QD40bU_M7CWrj9k0MEACNBRqwqVTSLDc -O scale_stats.npy\n",
        "        !sudo apt-get install espeak\n",
        "        !git clone https://github.com/coqui-ai/TTS\n",
        "\n",
        "        %cd TTS\n",
        "        !git checkout 540d811\n",
        "        !pip install -r requirements.txt\n",
        "        !python setup.py install\n",
        "\n",
        "        # sometimes installation does not work\n",
        "        import os, sys\n",
        "        sys.path.append(os.getcwd())\n",
        "        %cd ..\n",
        "    else:\n",
        "        !git clone https://github.com/1ucky40nc3/TransformerTTS.git\n",
        "        %cd TransformerTTS\n",
        "        !git checkout package\n",
        "        !pip install torchaudio\n",
        "        !pip install -r /content/TransformerTTS/requirements.txt\n",
        "        !pip install -r /content/TransformerTTS/TransformerTTS/vocoding/extra_requirements.txt\n",
        "        !python setup.py develop\n",
        "\n",
        "        !wget https://public-asai-dl-models.s3.eu-central-1.amazonaws.com/hifigan.zip\n",
        "        !unzip -q hifigan.zip\n",
        "        !rsync -avq hifigan/ /content/TransformerTTS/TransformerTTS/vocoding/hifigan/\n",
        "\n",
        "execute(install_tts_dependencies, verbose=VERBOSE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDXVw9toJ6JG",
        "cellView": "form"
      },
      "source": [
        "# @title | TTS | TTS Implementation\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown #### ‚ùó ***If \"de\" is selected as language an error may accure.***\n",
        "# @markdown #### ‚è© Just try to rerun this cell. üëª \n",
        "\n",
        "# @markdown ---\n",
        "USE_GPU_4_GERMAN_TTS = False # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchaudio import functional as F\n",
        "\n",
        "def text_to_speech_implementation(**kwargs):\n",
        "    if LANGUAGE == \"de\":\n",
        "        import os\n",
        "        from TTS.utils.io import load_config\n",
        "        from TTS.utils.audio import AudioProcessor\n",
        "        from TTS.tts.utils.io import load_checkpoint\n",
        "        from TTS.tts.utils.synthesis import synthesis\n",
        "        from TTS.tts.utils.text.symbols import symbols\n",
        "        from TTS.tts.utils.generic_utils import setup_model\n",
        "        from TTS.vocoder.utils.generic_utils import setup_generator\n",
        "        from TTS.vocoder.utils.io import load_checkpoint as load_vocoder_checkpoint\n",
        "\n",
        "        TTS_MODEL = \"/content/tts_model.pth.tar\"\n",
        "        TTS_CONFIG = \"/content/config.json\"\n",
        "        VOCODER_MODEL = \"/content/vocoder_model.pth.tar\"\n",
        "        VOCODER_CONFIG = \"/content/config_vocoder.json\"\n",
        "\n",
        "        TTS_CONFIG = load_config(TTS_CONFIG)\n",
        "        TTS_CONFIG.audio[\"stats_path\"] = \"/content/scale_stats.npy\"\n",
        "\n",
        "        VOCODER_CONFIG = load_config(VOCODER_CONFIG)\n",
        "\n",
        "        audio_processor = AudioProcessor(**TTS_CONFIG.audio)\n",
        "\n",
        "        model, _ = load_checkpoint(\n",
        "            setup_model(\n",
        "                num_chars=len(symbols), \n",
        "                num_speakers=0,\n",
        "                c=TTS_CONFIG),\n",
        "            checkpoint_path=TTS_MODEL)\n",
        "\n",
        "        vocoder, _ = load_vocoder_checkpoint(\n",
        "            setup_generator(VOCODER_CONFIG), \n",
        "            checkpoint_path=VOCODER_MODEL)\n",
        "        vocoder.remove_weight_norm()\n",
        "        vocoder.inference_padding = 0\n",
        "\n",
        "        if USE_GPU_4_GERMAN_TTS:\n",
        "            model.cuda()\n",
        "            vocoder.cuda()\n",
        "\n",
        "        model.eval()\n",
        "        vocoder.eval()\n",
        "\n",
        "        def text_to_speech(text: str) -> np.ndarray:\n",
        "            _, _, _, mel_postnet_spec, _, _ = synthesis(\n",
        "                model, \n",
        "                text, \n",
        "                TTS_CONFIG,\n",
        "                USE_GPU_4_GERMAN_TTS, \n",
        "                audio_processor)\n",
        "            \n",
        "            speech = vocoder.inference(\n",
        "                torch.FloatTensor(\n",
        "                    mel_postnet_spec.T,\n",
        "                ).unsqueeze(0))\n",
        "            speech = speech.flatten().cpu().numpy()\n",
        "\n",
        "            return speech\n",
        "        \n",
        "        return text_to_speech\n",
        "    \n",
        "    %cd /content/TransformerTTS\n",
        "\n",
        "    from TransformerTTS.model.factory import tts_ljspeech\n",
        "    from TransformerTTS.vocoding.predictors import HiFiGANPredictor\n",
        "\n",
        "\n",
        "    folder = \"/content/TransformerTTS/TransformerTTS/vocoding/hifigan/en\"\n",
        "\n",
        "\n",
        "    model, _ = tts_ljspeech()\n",
        "    vocoder = HiFiGANPredictor.from_folder(folder)\n",
        "\n",
        "    def text_to_speech(text: str) -> np.ndarray:\n",
        "        speech = model.predict(text)\n",
        "        speech = speech[\"mel\"].numpy().T\n",
        "        speech = vocoder([speech])[0]\n",
        "\n",
        "        return speech\n",
        "\n",
        "    %cd ..\n",
        "    return text_to_speech\n",
        "\n",
        "text_to_speech = execute(text_to_speech_implementation, verbose=VERBOSE)\n",
        "\n",
        "def postprocessing(wav: np.ndarray) -> np.ndarray:\n",
        "    wav = torch.from_numpy(wav)\n",
        "\n",
        "    wav = wav.unsqueeze(-1).T\n",
        "    wav = F.apply_codec(\n",
        "        waveform=wav, \n",
        "        sample_rate=22050,\n",
        "        format=\"wav\", \n",
        "        encoding=\"PCM_F\")\n",
        "    wav = F.resample(\n",
        "        waveform=wav, \n",
        "        orig_freq=22050, \n",
        "        new_freq=16000)\n",
        "\n",
        "    wav = wav.squeeze()\n",
        "    wav = wav.numpy()\n",
        "    \n",
        "    return wav"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgUr4Q6TJ6JH"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Avatar (PC-AVS)*** ü§óü§ñ\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hU2pN_5QGY-G"
      },
      "source": [
        "# @title | PC-AVS | Install Dependencies ‚á©\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "\n",
        "def install_avatar_dependencies(**kwargs):\n",
        "    !git clone https://github.com/1ucky40nc3/Talking-Face_PC-AVS.git\n",
        "    %cd /content/Talking-Face_PC-AVS\n",
        "\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install lws\n",
        "    !pip install face-alignment\n",
        "    !pip install av\n",
        "    !pip install torchaudio\n",
        "\n",
        "    !unzip ./misc/Audio_Source.zip -d ./misc/\n",
        "    !unzip ./misc/Input.zip -d ./misc/\n",
        "    !unzip ./misc/Mouth_Source.zip -d ./misc/ \n",
        "    !unzip ./misc/Pose_Source.zip -d ./misc/\n",
        "\n",
        "    !gdown https://drive.google.com/u/0/uc?id=1Zehr3JLIpzdg2S5zZrhIbpYPKF-4gKU_&export=download\n",
        "    !mkdir checkpoints\n",
        "    !unzip demo.zip -d ./checkpoints/\n",
        "\n",
        "execute(install_avatar_dependencies, verbose=VERBOSE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5yeHLAfXGvNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da5286a-7972-4f42-9eab-cbcd170b55d3"
      },
      "source": [
        "# @title | PC-AVS | PC-AVS Implementation\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "%cd /content/Talking-Face_PC-AVS\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "\n",
        "sys.path.append('..')\n",
        "\n",
        "from data import create_dataloader\n",
        "from models import create_model\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "\n",
        "def pc_avs_inference(opt, \n",
        "                     path_label, \n",
        "                     model, \n",
        "                     wav) -> str:\n",
        "    opt.path_label = path_label\n",
        "    dataloader = create_dataloader(opt, wav=wav)\n",
        "\n",
        "    fake_image_driven_pose_as = []\n",
        "\n",
        "    for data_i in tqdm(dataloader):\n",
        "        _, fake_image_driven_pose_a = model.forward(\n",
        "            data_i, mode='inference')\n",
        "\n",
        "        fake_image_driven_pose_as.append(\n",
        "            fake_image_driven_pose_a)\n",
        "\n",
        "    filename = os.path.join(\n",
        "        dataloader.dataset.get_processed_file_savepath(), \n",
        "        \"G_Pose_Driven_.mp4\")\n",
        "\n",
        "    video_array = torch.cat(fake_image_driven_pose_as, dim=0)\n",
        "    video_array = video_array.cpu().transpose(1, 3)\n",
        "    video_array = video_array * 125.5 + 125.5 \n",
        "    video_array = video_array.type(torch.uint8)\n",
        "    video_array = torch.rot90(video_array, -1, [1, 2])\n",
        "\n",
        "    wav = torch.from_numpy(wav)\n",
        "    wav = torch.unsqueeze(wav, dim=0)\n",
        "    \n",
        "    torchvision.io.write_video(\n",
        "        filename=filename, \n",
        "        video_array=video_array,\n",
        "        fps=25,\n",
        "        video_codec=\"libx264\",\n",
        "        audio_array=wav,\n",
        "        audio_fps=16000,\n",
        "        audio_codec=\"aac\"\n",
        "    )    \n",
        "\n",
        "    del dataloader\n",
        "    return filename\n",
        "\n",
        "\n",
        "def avatar(opt,\n",
        "           path_label,\n",
        "           wav) -> str:\n",
        "    opt.isTrain = False\n",
        "\n",
        "    model = create_model(opt).cuda()\n",
        "    model.eval()\n",
        "\n",
        "    return pc_avs_inference(\n",
        "        opt, \n",
        "        path_label, \n",
        "        model, \n",
        "        wav)\n",
        "    \n",
        "\n",
        "opt = Namespace(\n",
        "    D_input='single', \n",
        "    VGGFace_pretrain_path='', \n",
        "    aspect_ratio=1.0, \n",
        "    audio_nc=256, \n",
        "    augment_target=False, \n",
        "    batchSize=16, \n",
        "    beta1=0.5, \n",
        "    beta2=0.999, \n",
        "    checkpoints_dir='./checkpoints', \n",
        "    clip_len=1, \n",
        "    crop=False, \n",
        "    crop_len=16, \n",
        "    crop_size=224, \n",
        "    data_path='/home/SENSETIME/zhouhang1/Downloads/VoxCeleb2/voxceleb2_train.csv', \n",
        "    dataset_mode='voxtest', \n",
        "    defined_driven=False, \n",
        "    dis_feat_rec=False, \n",
        "    display_winsize=224, \n",
        "    driven_type='face', \n",
        "    driving_pose=True, \n",
        "    feature_encoded_dim=2560, \n",
        "    feature_fusion='concat', \n",
        "    filename_tmpl='{:06}.jpg', \n",
        "    fitting_iterations=10, \n",
        "    frame_interval=1, \n",
        "    frame_rate=25, \n",
        "    gan_mode='hinge', \n",
        "    gen_video=True, \n",
        "    generate_from_audio_only=True, \n",
        "    generate_interval=1, \n",
        "    gpu_ids=[0], \n",
        "    has_mask=False, \n",
        "    heatmap_size=3, \n",
        "    hop_size=160, \n",
        "    how_many=1000000, \n",
        "    init_type='xavier', \n",
        "    init_variance=0.02, \n",
        "    input_id_feature=True, \n",
        "    input_path='./checkpoints/results/input_path', \n",
        "    isTrain=False, \n",
        "    label_mask=False, \n",
        "    lambda_D=1, \n",
        "    lambda_contrastive=100, \n",
        "    lambda_crossmodal=1, \n",
        "    lambda_feat=10.0, \n",
        "    lambda_image=1.0, \n",
        "    lambda_rotate_D=0.1, \n",
        "    lambda_softmax=1000000, \n",
        "    lambda_vgg=10.0, \n",
        "    lambda_vggface=5.0, \n",
        "    landmark_align=False, \n",
        "    landmark_type='min', \n",
        "    list_end=1000000, \n",
        "    list_num=0, \n",
        "    list_start=0, \n",
        "    load_from_opt_file=False, \n",
        "    load_landmark=False, \n",
        "    lr=0.001, \n",
        "    lrw_data_path='/home/SENSETIME/zhouhang1/Downloads/VoxCeleb2/voxceleb2_train.csv', \n",
        "    max_dataset_size=9223372036854775807, \n",
        "    meta_path_vox='./conversations/feaa8fc7-8fc7-4ecf-acef-f06ca221b493/15/avatar.csv', \n",
        "    mode='cpu', \n",
        "    model='av', \n",
        "    multi_gpu=False, \n",
        "    nThreads=4, \n",
        "    n_mel_T=4, \n",
        "    name='demo', \n",
        "    ndf=64, \n",
        "    nef=16, \n",
        "    netA='resseaudio', \n",
        "    netA_sync='ressesync', \n",
        "    netD='multiscale', \n",
        "    netE='fan', \n",
        "    netG='modulate', \n",
        "    netV='resnext', \n",
        "    ngf=64, \n",
        "    no_TTUR=False, \n",
        "    no_flip=True, \n",
        "    no_ganFeat_loss=False, \n",
        "    no_gaussian_landmark=False, \n",
        "    no_id_loss=False, \n",
        "    no_instance=False, \n",
        "    no_pairing_check=False, \n",
        "    no_spectrogram=False, \n",
        "    no_vgg_loss=False, \n",
        "    noise_pose=True, \n",
        "    norm_A='spectralinstance', \n",
        "    norm_D='spectralinstance', \n",
        "    norm_E='spectralinstance', \n",
        "    norm_G='spectralinstance', \n",
        "    num_bins_per_frame=4, \n",
        "    num_classes=5830, \n",
        "    num_clips=1, \n",
        "    num_frames_per_clip=5, \n",
        "    num_inputs=1, \n",
        "    onnx=False, \n",
        "    optimizer='adam', \n",
        "    output_nc=3, \n",
        "    phase='test', \n",
        "    pose_dim=12, \n",
        "    positional_encode=False, \n",
        "    preprocess_mode='resize_and_crop', \n",
        "    results_dir='./conversations/feaa8fc7-8fc7-4ecf-acef-f06ca221b493/15', \n",
        "    save_path='./conversations/feaa8fc7-8fc7-4ecf-acef-f06ca221b493/15', \n",
        "    serial_batches=False, \n",
        "    start_ind=0, \n",
        "    style_dim=2560, \n",
        "    style_feature_loss=True, \n",
        "    target_crop_len=0, \n",
        "    train_dis_pose=False, \n",
        "    train_recognition=False, \n",
        "    train_sync=False, \n",
        "    train_word=False, \n",
        "    trainer='audio', \n",
        "    use_audio=1, \n",
        "    use_audio_id=0, \n",
        "    use_transformer=False, \n",
        "    verbose=False, \n",
        "    vgg_face=False, \n",
        "    which_epoch='latest', \n",
        "    word_loss=False\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Talking-Face_PC-AVS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG8kX8OHvA-U"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# ***T-REX*** ü¶ñ\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXxUibBOvZWT",
        "cellView": "form"
      },
      "source": [
        "# @title | T-REX | Start new Conversation\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "#@markdown ---\n",
        "ACTIVATE_PERSONAS = False # @param {type:\"boolean\"}\n",
        "PERSONA_1 = \"I work in a travel agency\" # @param {type:\"string\"}\n",
        "PERSONA_1 = f\"your persona: {PERSONA_1}\"\n",
        "PERSONA_2 = \"My name is Mia\" # @param {type:\"string\"}\n",
        "PERSONA_2 = f\"your persona: {PERSONA_2}\"\n",
        "\n",
        "#@markdown ---\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "\n",
        "import uuid\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def trex_setup(**kwargs):\n",
        "    config = {\n",
        "        \"languages\": LANGUAGES,\n",
        "        \"personas\": [PERSONA_1, PERSONA_2] if ACTIVATE_PERSONAS else [],\n",
        "        \"skills\": SKILLS,\n",
        "    }\n",
        "\n",
        "    nlp = NLP(config=config, **kwargs)\n",
        "\n",
        "    conversation_id = uuid.uuid4()\n",
        "    conversation_dir = f\"./conversations/{conversation_id}\"\n",
        "    !mkdir ./conversations/\n",
        "    !mkdir {conversation_dir}\n",
        "\n",
        "    interaction_counter = 0\n",
        "    f\"Current Conversation is logged at: {conversation_dir}\"\n",
        "\n",
        "    !rm -r /content/Talking-Face_PC-AVS/results/id_input_pose_00473_audio_tts_output\n",
        "\n",
        "    return nlp\n",
        "\n",
        "nlp = execute(trex_setup, verbose=VERBOSE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWlnRnDoJ_gd",
        "cellView": "form"
      },
      "source": [
        "# @title # T-REX ü¶ñ\n",
        "# @markdown ‚úã Rerun Cell if Runtime was restarted üîÑ\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Default: STT Input\n",
        "\n",
        "# States if STT output shall be used.\n",
        "RECORD_AUDIO = True # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Alternative: Text Input\n",
        "\n",
        "# Get the text input.\n",
        "TEXT_INPUT = \"Is the ICE 77 delayed?\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "verbose = \"\" if VERBOSE else \"&> /dev/null\"\n",
        "\n",
        "def trex(input: str=\"\", **kwargs) -> str:\n",
        "    print(f\"[DEBUG] |T-REX STT| STT Output: {input}\")\n",
        "\n",
        "    input = f'\"{input}\"'\n",
        "\n",
        "    output = nlp(input, **kwargs)\n",
        "    output = f'\"{output}\"'\n",
        "    print(f\"[DEBUG] |T-REX NLP| NLP Output: {output}\")\n",
        "\n",
        "    audio = text_to_speech(output)\n",
        "    audio = postprocessing(audio)\n",
        "\n",
        "    image_id = \"1\" if LANGUAGE == \"de\" else \"2\"\n",
        "    path_labels = f\"./misc/Input/input {image_id} ./misc/Pose_Source/00473 158 ./misc/Audio_Source/tts_output.mp3 None 0 None\"\n",
        "\n",
        "    print(path_labels)\n",
        "    video = avatar(\n",
        "        opt,\n",
        "        path_labels,\n",
        "        audio\n",
        "    )\n",
        "\n",
        "    return video\n",
        "\n",
        "input = speech_to_text(record()) if RECORD_AUDIO else TEXT_INPUT\n",
        "video = execute(trex, input=input, verbose=VERBOSE)\n",
        "\n",
        "# Show the final output.\n",
        "mp4 = open(video,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=700 controls autoplay>\n",
        "    <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TREX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO4tJI9q7GqjWQOutnSaENF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1ucky40nc3/TREX/blob/main/TREX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Zo09hxv31Y"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Speech Recognition (STT)***\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "N7WGmmWMbf4o"
      },
      "source": [
        "# @title | STT | Installation of Dependencies\n",
        "\n",
        "%%shell\n",
        "pip install transformers\n",
        "pip install ffmpeg-python\n",
        "pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PauhmEq8chkm",
        "cellView": "form"
      },
      "source": [
        "# @title | STT | Audio Recording Utils\n",
        "\"\"\"Utils for recording audio in a Google Colaboratory notebook.\n",
        "\n",
        "This code is adapted from:\n",
        "    https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/\n",
        "    https://colab.research.google.com/gist/ricardodeazambuja/03ac98c31e87caf284f7b06286ebf7fd/microphone-to-numpy-array-from-your-browser-in-colab.ipynb\n",
        "\"\"\"\n",
        "\n",
        "SILENT = \"&> /dev/null\"\n",
        "\n",
        "import io\n",
        "import ffmpeg\n",
        "import numpy as np\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import write\n",
        "from dl_colab_notebooks.audio import audio_bytes_to_np\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import HTML\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "\n",
        "STYLES_HTML = \"\"\"\n",
        "<script>\n",
        "\n",
        "var styles = `\n",
        "\n",
        ".output_html {\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    justify-content: center;\n",
        "    align-items: center;\n",
        "}\n",
        "\n",
        "button {\n",
        "    width: 300px;\n",
        "    height: 54px;\n",
        "\n",
        "    padding: 20px;\n",
        "    margin: 5px;\n",
        "\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "    align-items: center;\n",
        "    border-radius: 40px;\n",
        "    border: none;\n",
        "\n",
        "    text-align: center;\n",
        "    font-size: 28px;\n",
        "    \n",
        "    transition: all 0.5s;\n",
        "    cursor: pointer;\n",
        "}\n",
        "\n",
        "button span {\n",
        "    display: inline-block;\n",
        "    position: relative;\n",
        "\n",
        "    cursor: pointer;\n",
        "    transition: 0.5s;\n",
        "}\n",
        "\n",
        "button span:after {\n",
        "    content: 'üôè';\n",
        "\n",
        "    position: absolute;\n",
        "    right: -20px;\n",
        "\n",
        "    opacity: 0;\n",
        "    transition: 0.5s;\n",
        "}\n",
        "\n",
        "button:hover span {\n",
        "    padding-right: 25px;\n",
        "}\n",
        "\n",
        "button:hover span:after {\n",
        "    right: 0;\n",
        "    opacity: 1;\n",
        "}\n",
        "`\n",
        "\n",
        "var styleSheet = document.createElement(\"style\")\n",
        "styleSheet.type = \"text/css\"\n",
        "styleSheet.innerText = styles\n",
        "document.head.appendChild(styleSheet);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "\n",
        "var container = document.createElement(\"div\");\n",
        "var button = document.createElement(\"button\");\n",
        "var span = document.createElement(\"span\");\n",
        "\n",
        "button.appendChild(span);\n",
        "container.appendChild(button);\n",
        "document.body.appendChild(container);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader, recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "            mimeType : 'audio/webm;codecs=opus'\n",
        "    };            \n",
        "    recorder = new MediaRecorder(stream);\n",
        "    recorder.ondataavailable = function(e) {            \n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        container.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data); \n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "span.innerText = \"‚è∏Ô∏é\";\n",
        "button.style.verticalAlign = \"middle\";\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "function toggleRecording() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        span.innerText = \"‚úÖ\"\n",
        "    }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "    return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve => {\n",
        "    button.onclick = () => {\n",
        "        toggleRecording()\n",
        "\n",
        "        sleep(2000).then(() => {\n",
        "            resolve(base64data.toString())\n",
        "        });\n",
        "    }\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def record(sample_rate: int = 16000) -> str:\n",
        "    display(HTML(STYLES_HTML + AUDIO_HTML))\n",
        "    data = eval_js(\"data\")\n",
        "    \n",
        "    audio_bytes = b64decode(data.split(',')[1])\n",
        "    return audio_bytes_to_np(audio_bytes, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjsRML5VtFXY",
        "cellView": "form"
      },
      "source": [
        "# @title | STT | Wav2Vec2 Speech Recognition\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import Wav2Vec2Tokenizer\n",
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "\n",
        "# load model and tokenizer\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "wav2vec2 = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "\n",
        "def speech_to_text(audio: np.ndarray) -> List[str]:   \n",
        "    input_values = tokenizer(\n",
        "        [audio], \n",
        "        return_tensors=\"pt\", \n",
        "        padding=\"longest\"\n",
        "    ).input_values\n",
        "\n",
        "    logits = wav2vec2(input_values).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    text = tokenizer.batch_decode(predicted_ids)\n",
        "    text = \" \".join(text)\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0AFeWgzwFgr"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Natural Language Processing (NLP)***\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC4zn53LYTw9",
        "cellView": "form"
      },
      "source": [
        "# @title | NLP | Install Dependencies\n",
        "\n",
        "%%shell\n",
        "pip install transformers\n",
        "pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1U9U9gdwKig",
        "cellView": "form"
      },
      "source": [
        "# @title | NLP | Initialize the NLP Pipelines\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "\n",
        "DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "\n",
        "ZERO_SHOT = transformers.pipeline(\"zero-shot-classification\", device=DEVICE)\n",
        "TABLE_QA = transformers.pipeline(\"table-question-answering\", model=\"google/tapas-large-finetuned-wikisql-supervised\")\n",
        "\n",
        "SMALL_TALK = transformers.pipeline(\"conversational\", model=\"facebook/blenderbot-400M-distill\", device=DEVICE)\n",
        "FEW_SHOT = transformers.pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B', device=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4aTN97_w5sH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "cellView": "form",
        "outputId": "f15b24a9-189c-4f36-b3bb-938218b43333"
      },
      "source": [
        "# @title | NLP | Set up Services for Data\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "TRAIN_TABLE = \"\"\"Location,Train,Start,Destination,Departure,Arrival\n",
        "Munich,ICE 77,Munich,Berlin,12:30,13:33\n",
        "Munich,ICE 56,Munich,Leipzig,12:30,13:30\n",
        "Munich,ICE 33,Leipzig,Berlin,13:30,14:00\"\"\"\n",
        "\n",
        "PLANE_TABLE = \"\"\"Location,Plane,Start,Destination,Departure,Arrival\n",
        "Munich,Eurowings (EW8003),Munich,Berlin,12:30,13:33\n",
        "Munich,Lufthansa (LH1940),Munich,Leipzig,12:30,13:30\n",
        "Munich,Lufthansa (LH2040),Leipzig,Berlin,13:30,14:00\"\"\"\n",
        "\n",
        "def train_table() -> pd.DataFrame:\n",
        "    return pd.read_csv(io.StringIO(TRAIN_TABLE))\n",
        "\n",
        "def plane_table() -> pd.DataFrame:\n",
        "    return pd.read_csv(io.StringIO(PLANE_TABLE))\n",
        "\n",
        "def df_to_csv(df) -> str:\n",
        "    csv = io.StringIO()\n",
        "    df.to_csv(csv, index=False)\n",
        "    return csv.getvalue()\n",
        "\n",
        "def csv_to_df(csv) -> pd.DataFrame:\n",
        "    return pd.read_csv(io.StringIO(csv))\n",
        "\n",
        "def travel_tables(tables=[train_table(), plane_table()]) -> pd.DataFrame:\n",
        "    travel = pd.concat(tables)\n",
        "    travel = travel.fillna(\"NONE\")\n",
        "\n",
        "    csv = df_to_csv(travel)\n",
        "    travel = csv_to_df(csv)\n",
        "    return travel\n",
        "\n",
        "travel_samples = \"\"\"Question: \"I am in Hannover. It is 17:48. Which train can I take from Munich to Berlin?\" Context: \"ICE 33\" Answer: \"The train ICE 33 will travel from Munich to Berlin.\"\n",
        "Question: \"I am in Frankfurt. It is 09:32. When will the next flight to Madrid leave?\" Context: \"07:59\" Answer: \"The next to Madrid will leave 07:59.\"\n",
        "Question: \"I am in Hannover. It is 17:48. What is the best train to get to Berlin from London?\" Context: \"ICE 33\" Answer: \"The train ICE 33 is the fastest.\"\n",
        "Question: \"I am in Munich. It is 05:51. From which platform does the RB 61 exit to Zurich?\" Context: \"8\" Answer: \"The RB 61 will departs from the platform 8.\"\n",
        "Question: \"I am in Berlin. It is 12:20. How can I get to London by aircraft?\" Context: \"Eurowings (EW8003)\" Answer: \"The flight Eurowings (EW8003) will leave for London.\"\n",
        "Question: \"I am in London. It is 12:20. I am at a train station. How can I get to Frankfurt?\" Context: \"ICE 923\" Answer: \"The train ICE 77 is available for your trip.\"\n",
        "Question: \"I am in Hamburg. It is 23:43. From which gate does the Lufthansa aircraft (LH1940) take off?\" Context: \"23\" Answer: \"The Lufthansa airliner (LH1940) will take off from gate 23.\"\n",
        "Question: \"I am in Stuttgart. It is 08:24. When will the next ICE to Bremen leave?\" Context: \"07:59\" Answer: \"The next to Bremen will leave 07:59.\"\n",
        "\"\"\"\n",
        "\n",
        "def travel_time() -> str:\n",
        "    return \"13:21\"\n",
        "\n",
        "def travel_location() -> str:\n",
        "    return \"Munich\"\n",
        "\n",
        "travel_tables()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Location</th>\n",
              "      <th>Train</th>\n",
              "      <th>Start</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Departure</th>\n",
              "      <th>Arrival</th>\n",
              "      <th>Plane</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Munich</td>\n",
              "      <td>ICE 77</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:33</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Munich</td>\n",
              "      <td>ICE 56</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:30</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Munich</td>\n",
              "      <td>ICE 33</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>13:30</td>\n",
              "      <td>14:00</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Munich</td>\n",
              "      <td>NONE</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:33</td>\n",
              "      <td>Eurowings (EW8003)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Munich</td>\n",
              "      <td>NONE</td>\n",
              "      <td>Munich</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>12:30</td>\n",
              "      <td>13:30</td>\n",
              "      <td>Lufthansa (LH1940)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Munich</td>\n",
              "      <td>NONE</td>\n",
              "      <td>Leipzig</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>13:30</td>\n",
              "      <td>14:00</td>\n",
              "      <td>Lufthansa (LH2040)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Location   Train    Start Destination Departure Arrival               Plane\n",
              "0   Munich  ICE 77   Munich      Berlin     12:30   13:33                NONE\n",
              "1   Munich  ICE 56   Munich     Leipzig     12:30   13:30                NONE\n",
              "2   Munich  ICE 33  Leipzig      Berlin     13:30   14:00                NONE\n",
              "3   Munich    NONE   Munich      Berlin     12:30   13:33  Eurowings (EW8003)\n",
              "4   Munich    NONE   Munich     Leipzig     12:30   13:30  Lufthansa (LH1940)\n",
              "5   Munich    NONE  Leipzig      Berlin     13:30   14:00  Lufthansa (LH2040)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDT5V9m2w3sQ",
        "cellView": "form"
      },
      "source": [
        "# @title | NLP | NLP Implementation\n",
        "\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Tuple\n",
        "from typing import Callable\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "class Associations(Dict):\n",
        "    data: Callable[[], pd.DataFrame]\n",
        "    samples: str\n",
        "\n",
        "Response = Tuple[str, Optional[transformers.Conversation]]\n",
        "\n",
        "Function = Callable[\n",
        "    [transformers.Conversation], \n",
        "    Response\n",
        "]\n",
        "\n",
        "class Skill(Dict):\n",
        "    associations: Associations\n",
        "    function: Function\n",
        "\n",
        "class Skills(Dict):\n",
        "    name: str\n",
        "    skill: Skill\n",
        "\n",
        "WarmUp = Callable[\n",
        "    [transformers.Conversation], \n",
        "    transformers.Conversation\n",
        "]\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Classification on a Zero-Shot basis.\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def zero_shot_classification(input: str, \n",
        "                             labels: List[str], \n",
        "                             top_k: Optional[int] = 1,\n",
        "                             **kwargs) -> List[str]:\n",
        "    return ZERO_SHOT(input, labels, **kwargs)[\"labels\"][:top_k]\n",
        "\n",
        "def skill_classification(input: str, \n",
        "                         skills: List[str], \n",
        "                         verbose: Optional[bool],\n",
        "                         **kwargs) -> List[str]:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Skill Classification| input: {input}\")\n",
        "        print(f\"[DEBUG] |Skill Classification| skills: {skills}\")\n",
        "\n",
        "    skill = zero_shot_classification(input, skills, **kwargs)[0]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Skill Classification| skill: {skill}\")\n",
        "    return skill\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~\n",
        "Section for Table QA.\n",
        "~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def table_question_answering(input: str, \n",
        "                             table: pd.DataFrame, \n",
        "                             verbose: Optional[bool] = False, \n",
        "                             **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Table Question Answering| input: {input}\")\n",
        "        print(f\"[DEBUG] |Table Question Answering| table: \\n{table}\")\n",
        "    \n",
        "    output = TABLE_QA(table=table, query=input)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Table Question Answering| output: \\n{output}\")\n",
        "    return output[\"answer\"]\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Few-Shot Text Generation\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def few_shot(query: str, \n",
        "             samples: str, \n",
        "             verbose: Optional[bool] = False, \n",
        "             **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Few-Shot Text Generation| query: \\n{query}\")\n",
        "        print(f\"[DEBUG] |Few-Shot Text Generation| samples: \\n{samples}\")\n",
        "    \n",
        "    output = FEW_SHOT(samples + query, **kwargs)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Few-Shot Text Generation| output: \\n{output}\")\n",
        "    return output[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "def travel_few_shot(query: str, \n",
        "                    samples: str, \n",
        "                    verbose: Optional[bool] = False, \n",
        "                    **kwargs) -> str:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Few-Shot Text Generation| query: \\n{query}\")\n",
        "        print(f\"[DEBUG] |Travel Few-Shot Text Generation| samples: \\n{samples}\")\n",
        "\n",
        "    output = few_shot(query, \n",
        "                      samples, \n",
        "                      verbose, \n",
        "                      **kwargs)\n",
        "    output = output[len(samples + query):]\n",
        "    output = output.split('\"')[0]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Few-Shot Text Generation| output: \\n{output}\")\n",
        "    return output\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Skill Functions\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "def travel_skill(conversation: transformers.Conversation, \n",
        "                 associations: Associations, \n",
        "                 verbose: Optional[bool] = False, \n",
        "                 **kwargs) -> Response:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Skill| input conversation: \\n{conversation}\")\n",
        "        print(f\"[DEBUG] |Travel Skill| associations: \\n{associations}\")\n",
        "\n",
        "    input = conversation.new_user_input\n",
        "    labels = list(associations.keys())\n",
        "\n",
        "    variant = zero_shot_classification(input, labels, **kwargs)[0]\n",
        "\n",
        "    data = associations[variant][\"data\"]()\n",
        "    time = associations[variant][\"time\"]()\n",
        "    location = associations[variant][\"location\"]()\n",
        "    samples = associations[variant][\"samples\"]\n",
        "    config = associations[variant][\"config\"]\n",
        "\n",
        "    cell = table_question_answering(input, data, verbose)\n",
        "\n",
        "    input = f\"I am in {location}. It is {time}. {input}\"\n",
        "    query = f'Question: \"{input}\" Context: \"{cell}\" Answer: \"'\n",
        "\n",
        "    output = travel_few_shot(\n",
        "        query, \n",
        "        samples, \n",
        "        verbose, \n",
        "        **{**config, **kwargs})\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Travel Skill| variant: {variant}\")\n",
        "        print(f\"[DEBUG] |Travel Skill| cell: {cell}\")\n",
        "        print(f\"[DEBUG] |Travel Skill| output: {output}\")\n",
        "    return output, None\n",
        "\n",
        "def small_talk_skill(conversation: transformers.Conversation, \n",
        "                     verbose: Optional[bool] = False, \n",
        "                     **kwargs) -> Response:\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Small Talk Skill| input conversation: \\n{conversation}\")\n",
        "\n",
        "    conversation = SMALL_TALK(conversation, **kwargs)\n",
        "    output = conversation.generated_responses[-1]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Small Talk Skill| output conversation: \\n{conversation}\")\n",
        "        print(f\"[DEBUG] |Small Talk Skill| output: {output}\")\n",
        "    return output, conversation\n",
        "\n",
        "\n",
        "\"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Section for Personas and Warm Up\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
        "\n",
        "PERSONAS = [\n",
        "    \"your persona: i am a travel assistant\",\n",
        "]\n",
        "\n",
        "def warm_up(conversation: transformers.Conversation, \n",
        "            personas: List[str] = PERSONAS, \n",
        "            verbose: bool = False,\n",
        "            **kwargs) -> transformers.Conversation:\n",
        "    for persona in personas:\n",
        "        conversation.add_user_input(persona)\n",
        "        conversation = SMALL_TALK(conversation, **kwargs)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[DEBUG] |Warm Up| personas: {personas}\")\n",
        "        print(f\"[DEBUG] |Warm Up| personas: {conversation}\")\n",
        "    return conversation\n",
        "\n",
        "\n",
        "\n",
        "SKILLS: Skills = {\n",
        "    \"small talk\": {\n",
        "        \"associations\": {}, \n",
        "        \"function\": small_talk_skill\n",
        "    },\n",
        "    \"travel\": {\n",
        "        \"associations\": {\n",
        "            \"travel\": {\n",
        "                \"data\": travel_tables,\n",
        "                \"time\": travel_time,\n",
        "                \"location\": travel_location,\n",
        "                \"samples\": travel_samples,\n",
        "                \"config\": {\n",
        "                    \"temperature\": 0.6,\n",
        "                    \"do_sample\": True,\n",
        "                    \"max_length\": 475,\n",
        "                }\n",
        "            }\n",
        "        }, \n",
        "        \"function\": travel_skill\n",
        "    },\n",
        "    \"other\": {\n",
        "        \"associations\": {}, \n",
        "        \"function\": small_talk_skill\n",
        "    },\n",
        "}\n",
        "\n",
        "CONFIG = {\n",
        "    \"skills\": SKILLS,\n",
        "    \"personas\": PERSONAS\n",
        "} \n",
        "\n",
        "\n",
        "class NLP:\n",
        "    def __init__(self,\n",
        "                 config: dict = CONFIG,\n",
        "                 verbose: bool = True,\n",
        "                 **kwargs):\n",
        "        self.config = config\n",
        "        self.skills = CONFIG[\"skills\"]\n",
        "\n",
        "        self.conversation = transformers.Conversation()\n",
        "        self.conversation = warm_up(\n",
        "            self.conversation, \n",
        "            personas=config[\"personas\"],\n",
        "            verbose=verbose,\n",
        "            **kwargs)\n",
        "\n",
        "    def __call__(self, input: str, verbose: bool = False, **kwargs):\n",
        "        if verbose:\n",
        "            print(f\"[DEBUG] |NLP __call__ START|\" + \"~\"*20)\n",
        "            print(f\"[DEBUG] |NLP ATTR skills|: {self.skills}\")\n",
        "            print(f\"[DEBUG] |NLP ATTR conversation|: \\n{self.conversation}\")\n",
        "            print(f\"[DEBUG] |NLP User input|: {input}\")\n",
        "\n",
        "        self.conversation.add_user_input(input)\n",
        "\n",
        "        skill = skill_classification(\n",
        "            input, \n",
        "            list(self.skills.keys()), \n",
        "            verbose=verbose, \n",
        "            **kwargs)\n",
        "        \n",
        "        pipeline = self.skills[skill]\n",
        "        function = pipeline[\"function\"]\n",
        "        associations = pipeline[\"associations\"]\n",
        "        \n",
        "        output, conversation = function(\n",
        "            self.conversation, \n",
        "            associations=associations, \n",
        "            verbose=verbose,\n",
        "            **kwargs)\n",
        "\n",
        "        if conversation:\n",
        "            self.conversation = conversation\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[DEBUG] |NLP User output|: {output}\")\n",
        "            print(f\"[DEBUG] |NLP __call__ END|\" + \"~\"*20)\n",
        "        return output\n",
        "\n",
        "nlp = NLP()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXF876tBJ6I8"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Text-To-Speech (TTS)***\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5UxGOjsiJ6JF"
      },
      "source": [
        "# @title | TTS | Installation of Dependencies\n",
        "\n",
        "!apt-get install -y espeak\n",
        "\n",
        "!git clone https://github.com/1ucky40nc3/TransformerTTS.git\n",
        "%cd TransformerTTS\n",
        "!git checkout package\n",
        "!pip install torchaudio\n",
        "!pip install -r /content/TransformerTTS/requirements.txt\n",
        "!pip install -r /content/TransformerTTS/TransformerTTS/vocoding/extra_requirements.txt\n",
        "!python setup.py develop\n",
        "\n",
        "!wget https://public-asai-dl-models.s3.eu-central-1.amazonaws.com/hifigan.zip\n",
        "!unzip -q hifigan.zip\n",
        "!rsync -avq hifigan/ /content/TransformerTTS/TransformerTTS/vocoding/hifigan/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yDXVw9toJ6JG"
      },
      "source": [
        "# @title | TTS | TTS Implementation\n",
        "%cd /content/TransformerTTS\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchaudio import functional as F\n",
        "\n",
        "from TransformerTTS.model.factory import tts_ljspeech\n",
        "from TransformerTTS.vocoding.predictors import HiFiGANPredictor\n",
        "\n",
        "\n",
        "folder = \"/content/TransformerTTS/TransformerTTS/vocoding/hifigan/en\"\n",
        "\n",
        "\n",
        "model, _ = tts_ljspeech()\n",
        "vocoder = HiFiGANPredictor.from_folder(folder)\n",
        "\n",
        "\n",
        "def text_to_speech(text: str) -> np.ndarray:\n",
        "    speech = model.predict(text)\n",
        "    speech = speech[\"mel\"].numpy().T\n",
        "    speech = vocoder([speech])[0]\n",
        "\n",
        "    return speech\n",
        "\n",
        "%cd ..\n",
        "\n",
        "def postprocessing(wav: np.ndarray) -> np.ndarray:\n",
        "    wav = torch.from_numpy(wav)\n",
        "\n",
        "    wav = wav.unsqueeze(-1).T\n",
        "    wav = F.apply_codec(\n",
        "        waveform=wav, \n",
        "        sample_rate=22050,\n",
        "        format=\"wav\", \n",
        "        encoding=\"PCM_F\")\n",
        "    wav = F.resample(\n",
        "        waveform=wav, \n",
        "        orig_freq=22050, \n",
        "        new_freq=16000)\n",
        "\n",
        "    wav = wav.squeeze()\n",
        "    wav = wav.numpy()\n",
        "    \n",
        "    return wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgUr4Q6TJ6JH"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "# ***Avatar (PC-AVS)***\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hU2pN_5QGY-G"
      },
      "source": [
        "# @title | PC-AVS | Install Dependencies\n",
        "\n",
        "!git clone https://github.com/1ucky40nc3/Talking-Face_PC-AVS.git\n",
        "%cd /content/Talking-Face_PC-AVS\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install lws\n",
        "!pip install face-alignment\n",
        "!pip install av\n",
        "!pip install torchaudio\n",
        "\n",
        "!unzip ./misc/Audio_Source.zip -d ./misc/\n",
        "!unzip ./misc/Input.zip -d ./misc/\n",
        "!unzip ./misc/Mouth_Source.zip -d ./misc/ \n",
        "!unzip ./misc/Pose_Source.zip -d ./misc/\n",
        "\n",
        "!gdown https://drive.google.com/u/0/uc?id=1Zehr3JLIpzdg2S5zZrhIbpYPKF-4gKU_&export=download\n",
        "!mkdir checkpoints\n",
        "!unzip demo.zip -d ./checkpoints/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5yeHLAfXGvNw"
      },
      "source": [
        "# @title | PC-AVS | PC-AVS Implementation\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "\n",
        "sys.path.append('..')\n",
        "\n",
        "from data import create_dataloader\n",
        "from models import create_model\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "\n",
        "def pc_avs_inference(opt, \n",
        "                     path_label, \n",
        "                     model, \n",
        "                     wav) -> str:\n",
        "    opt.path_label = path_label\n",
        "    dataloader = create_dataloader(opt, wav=wav)\n",
        "\n",
        "    fake_image_driven_pose_as = []\n",
        "\n",
        "    for data_i in tqdm(dataloader):\n",
        "        _, fake_image_driven_pose_a = model.forward(\n",
        "            data_i, mode='inference')\n",
        "\n",
        "        fake_image_driven_pose_as.append(\n",
        "            fake_image_driven_pose_a)\n",
        "\n",
        "    filename = os.path.join(\n",
        "        dataloader.dataset.get_processed_file_savepath(), \n",
        "        \"G_Pose_Driven_.mp4\")\n",
        "\n",
        "    video_array = torch.cat(fake_image_driven_pose_as, dim=0)\n",
        "    video_array = video_array.cpu().transpose(1, 3)\n",
        "    video_array = video_array * 125.5 + 125.5 \n",
        "    video_array = video_array.type(torch.uint8)\n",
        "    video_array = torch.rot90(video_array, -1, [1, 2])\n",
        "\n",
        "    wav = torch.from_numpy(wav)\n",
        "    wav = torch.unsqueeze(wav, dim=0)\n",
        "    \n",
        "    torchvision.io.write_video(\n",
        "        filename=filename, \n",
        "        video_array=video_array,\n",
        "        fps=25,\n",
        "        video_codec=\"libx264\",\n",
        "        audio_array=wav,\n",
        "        audio_fps=16000,\n",
        "        audio_codec=\"aac\"\n",
        "    )    \n",
        "\n",
        "    del dataloader\n",
        "    return filename\n",
        "\n",
        "\n",
        "def avatar(opt,\n",
        "           path_label,\n",
        "           wav) -> str:\n",
        "    opt.isTrain = False\n",
        "\n",
        "    model = create_model(opt).cuda()\n",
        "    model.eval()\n",
        "\n",
        "    return pc_avs_inference(\n",
        "        opt, \n",
        "        path_label, \n",
        "        model, \n",
        "        wav)\n",
        "    \n",
        "\n",
        "opt = Namespace(\n",
        "    D_input='single', \n",
        "    VGGFace_pretrain_path='', \n",
        "    aspect_ratio=1.0, \n",
        "    audio_nc=256, \n",
        "    augment_target=False, \n",
        "    batchSize=16, \n",
        "    beta1=0.5, \n",
        "    beta2=0.999, \n",
        "    checkpoints_dir='./checkpoints', \n",
        "    clip_len=1, \n",
        "    crop=False, \n",
        "    crop_len=16, \n",
        "    crop_size=224, \n",
        "    data_path='/home/SENSETIME/zhouhang1/Downloads/VoxCeleb2/voxceleb2_train.csv', \n",
        "    dataset_mode='voxtest', \n",
        "    defined_driven=False, \n",
        "    dis_feat_rec=False, \n",
        "    display_winsize=224, \n",
        "    driven_type='face', \n",
        "    driving_pose=True, \n",
        "    feature_encoded_dim=2560, \n",
        "    feature_fusion='concat', \n",
        "    filename_tmpl='{:06}.jpg', \n",
        "    fitting_iterations=10, \n",
        "    frame_interval=1, \n",
        "    frame_rate=25, \n",
        "    gan_mode='hinge', \n",
        "    gen_video=True, \n",
        "    generate_from_audio_only=True, \n",
        "    generate_interval=1, \n",
        "    gpu_ids=[0], \n",
        "    has_mask=False, \n",
        "    heatmap_size=3, \n",
        "    hop_size=160, \n",
        "    how_many=1000000, \n",
        "    init_type='xavier', \n",
        "    init_variance=0.02, \n",
        "    input_id_feature=True, \n",
        "    input_path='./checkpoints/results/input_path', \n",
        "    isTrain=False, \n",
        "    label_mask=False, \n",
        "    lambda_D=1, \n",
        "    lambda_contrastive=100, \n",
        "    lambda_crossmodal=1, \n",
        "    lambda_feat=10.0, \n",
        "    lambda_image=1.0, \n",
        "    lambda_rotate_D=0.1, \n",
        "    lambda_softmax=1000000, \n",
        "    lambda_vgg=10.0, \n",
        "    lambda_vggface=5.0, \n",
        "    landmark_align=False, \n",
        "    landmark_type='min', \n",
        "    list_end=1000000, \n",
        "    list_num=0, \n",
        "    list_start=0, \n",
        "    load_from_opt_file=False, \n",
        "    load_landmark=False, \n",
        "    lr=0.001, \n",
        "    lrw_data_path='/home/SENSETIME/zhouhang1/Downloads/VoxCeleb2/voxceleb2_train.csv', \n",
        "    max_dataset_size=9223372036854775807, \n",
        "    meta_path_vox='./conversations/feaa8fc7-8fc7-4ecf-acef-f06ca221b493/15/avatar.csv', \n",
        "    mode='cpu', \n",
        "    model='av', \n",
        "    multi_gpu=False, \n",
        "    nThreads=4, \n",
        "    n_mel_T=4, \n",
        "    name='demo', \n",
        "    ndf=64, \n",
        "    nef=16, \n",
        "    netA='resseaudio', \n",
        "    netA_sync='ressesync', \n",
        "    netD='multiscale', \n",
        "    netE='fan', \n",
        "    netG='modulate', \n",
        "    netV='resnext', \n",
        "    ngf=64, \n",
        "    no_TTUR=False, \n",
        "    no_flip=True, \n",
        "    no_ganFeat_loss=False, \n",
        "    no_gaussian_landmark=False, \n",
        "    no_id_loss=False, \n",
        "    no_instance=False, \n",
        "    no_pairing_check=False, \n",
        "    no_spectrogram=False, \n",
        "    no_vgg_loss=False, \n",
        "    noise_pose=True, \n",
        "    norm_A='spectralinstance', \n",
        "    norm_D='spectralinstance', \n",
        "    norm_E='spectralinstance', \n",
        "    norm_G='spectralinstance', \n",
        "    num_bins_per_frame=4, \n",
        "    num_classes=5830, \n",
        "    num_clips=1, \n",
        "    num_frames_per_clip=5, \n",
        "    num_inputs=1, \n",
        "    onnx=False, \n",
        "    optimizer='adam', \n",
        "    output_nc=3, \n",
        "    phase='test', \n",
        "    pose_dim=12, \n",
        "    positional_encode=False, \n",
        "    preprocess_mode='resize_and_crop', \n",
        "    results_dir='./conversations/feaa8fc7-8fc7-4ecf-acef-f06ca221b493/15', \n",
        "    save_path='./conversations/feaa8fc7-8fc7-4ecf-acef-f06ca221b493/15', \n",
        "    serial_batches=False, \n",
        "    start_ind=0, \n",
        "    style_dim=2560, \n",
        "    style_feature_loss=True, \n",
        "    target_crop_len=0, \n",
        "    train_dis_pose=False, \n",
        "    train_recognition=False, \n",
        "    train_sync=False, \n",
        "    train_word=False, \n",
        "    trainer='audio', \n",
        "    use_audio=1, \n",
        "    use_audio_id=0, \n",
        "    use_transformer=False, \n",
        "    verbose=False, \n",
        "    vgg_face=False, \n",
        "    which_epoch='latest', \n",
        "    word_loss=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG8kX8OHvA-U"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# ***T-REX*** ü¶ñ\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXxUibBOvZWT",
        "cellView": "form"
      },
      "source": [
        "# @title | T-REX | Start new Conversation\n",
        "\n",
        "import uuid\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "NLP_PERSONA = False # @param {type:\"boolean\"}\n",
        "nlp = NLP() if NLP_PERSONA else NLP(config={\"skills\": SKILLS, \"personas\": []})\n",
        "\n",
        "conversation_id = uuid.uuid4()\n",
        "conversation_dir = f\"./conversations/{conversation_id}\"\n",
        "!mkdir ./conversations/\n",
        "!mkdir {conversation_dir}\n",
        "\n",
        "interaction_counter = 0\n",
        "f\"Current Conversation is logged at: {conversation_dir}\"\n",
        "\n",
        "!rm -r /content/Talking-Face_PC-AVS/results/id_input_pose_00473_audio_tts_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWlnRnDoJ_gd",
        "cellView": "form"
      },
      "source": [
        "# @title # T-REX ü¶ñ\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Default: STT Input\n",
        "\n",
        "# States if STT output shall be used.\n",
        "RECORD_AUDIO = False # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Alternative: Text Input\n",
        "\n",
        "# Get the text input.\n",
        "TEXT_INPUT = \"Hello. How are you today?\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "verbose = \"\" if VERBOSE else \"&> /dev/null\"\n",
        "\n",
        "interaction_dir = f\"{conversation_dir}/{interaction_counter}\"\n",
        "!mkdir {interaction_dir}\n",
        "interaction_counter += 1\n",
        "\n",
        "\n",
        "input = speech_to_text(record()) if RECORD_AUDIO else TEXT_INPUT\n",
        "print(f\"[DEBUG] |T-REX STT| Input for NLU: {input}\")\n",
        "\n",
        "input = f'\"{input}\"'\n",
        "\n",
        "output = nlp(input, VERBOSE)\n",
        "output = f'\"{output}\"'\n",
        "print(f\"[DEBUG] |T-REX NLP| NLP Output: {output}\")\n",
        "\n",
        "audio = text_to_speech(output)\n",
        "audio = postprocessing(audio)\n",
        "\n",
        "video = avatar(\n",
        "    opt,\n",
        "    \"./misc/Input/input 1 ./misc/Pose_Source/00473 158 ./misc/Audio_Source/tts_output.mp3 None 0 None\",\n",
        "    audio\n",
        ")\n",
        "\n",
        "# Show the final output.\n",
        "mp4 = open(video,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
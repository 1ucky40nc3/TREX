{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generation_research.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4EQ8q17zbwRJe4aE28UZI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1ucky40nc3/TREX/blob/research/generation_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUltTIrOaSK"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk9N_cMSOjGc"
      },
      "source": [
        "import torch\n",
        "\n",
        "from transformers import GPTNeoForCausalLM\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "from typing import List\n",
        "from typing import Union\n",
        "from typing import Any"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnRW3c12Pc9f"
      },
      "source": [
        "class GPTNeoGenerator:\n",
        "    def __init__(self, name, device=None):\n",
        "        self.name = name\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(name)\n",
        "        self.model = GPTNeoForCausalLM.from_pretrained(\n",
        "            name, pad_token_id=self.tokenizer.eos_token_id)\n",
        "\n",
        "        self.device = device if device else self.get_device()\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def __call__(self, input: str, **kwargs) -> List[str]:\n",
        "        input = self.tokenizer(\n",
        "            input, return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "\n",
        "        tokens = self.model.generate(\n",
        "            **{**input, **kwargs})\n",
        "        \n",
        "        tokens = self.tokenizer.batch_decode(\n",
        "            tokens, **kwargs)\n",
        "        return tokens\n",
        "\n",
        "    def greedy(self, \n",
        "               input: str,\n",
        "               num_return_sequences: str=1,\n",
        "               **kwargs) -> List[str]:\n",
        "        kwargs = {**locals(), **kwargs}\n",
        "        del kwargs[\"self\"]\n",
        "        del kwargs[\"num_return_sequences\"]\n",
        "\n",
        "        return [self.__call__(**kwargs)[0] for _ in range(num_return_sequences)]\n",
        "\n",
        "    def beam(self,\n",
        "             input: str,\n",
        "             num_beams: int,\n",
        "             early_stopping: bool=True,\n",
        "             **kwargs) -> List[str]:\n",
        "        kwargs = {**locals(), **kwargs}\n",
        "        del kwargs[\"self\"]\n",
        "\n",
        "        return self.__call__(**kwargs)\n",
        "\n",
        "    def sample(self,\n",
        "               input: str,\n",
        "               top_k: int=0,\n",
        "               do_sample: bool=True,\n",
        "               **kwargs) -> List[str]:\n",
        "        kwargs = {**locals(), **kwargs}\n",
        "        del kwargs[\"self\"]\n",
        "\n",
        "        return self.__call__(**kwargs)\n",
        "\n",
        "    def top_k(self,\n",
        "              input: str,\n",
        "              top_k: int,\n",
        "              **kwargs) -> List[str]:\n",
        "        kwargs = {**locals(), **kwargs}\n",
        "        del kwargs[\"self\"]\n",
        "\n",
        "        return self.sample(**kwargs)\n",
        "\n",
        "    def top_p(self,\n",
        "              input: str,\n",
        "              top_p: float,\n",
        "              **kwargs) -> List[str]:\n",
        "        kwargs = {**locals(), **kwargs}\n",
        "        del kwargs[\"self\"]\n",
        "\n",
        "        return self.sample(**kwargs)\n",
        "\n",
        "    def get_device(self) -> str:\n",
        "        return \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBHmhXG-TZHM"
      },
      "source": [
        "gpt_neo_125m = GPTNeoGenerator(\"EleutherAI/gpt-neo-125M\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7399B5GT7ue",
        "outputId": "22424ef2-9f4f-443e-ec77-13f734b862b7"
      },
      "source": [
        "gpt_neo_125m(\"Hello\", \n",
        "             max_length=100,\n",
        "             num_beams=5,\n",
        "             num_return_sequences=5, \n",
        "             temperature=0.1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone, I’m so excited to announce that I’m going to be writing this blog post. I’ve been working on this blog for a while now, and I’m excited to share it with you.\\n\\nI’ve been working on this blog for a while now, and I’m excited to share it with you. I’ve been working on this blog for a while now, and I’m excited to share',\n",
              " 'Hello everyone, I’m so excited to announce that I’m going to be writing this blog post. I’ve been working on this for a while now, and I’ve been working on it for a while now. I’ve been working on it for a while now, and I’ve been working on it for a while now. I’ve been working on it for a while now, and I’ve been working on it',\n",
              " 'Hello everyone, I’m so excited to announce that I’m going to be writing this blog post. I’ve been working on this for a while now, and I’ve been working on it for a while now. I’ve been working on this for a while now, and I’ve been working on it for a while now. I’ve been working on this for a while now, and I’ve been working on it',\n",
              " 'Hello everyone, I’m so excited to announce that I’m going to be writing this blog post. I’ve been working on this blog for a while now, and I’ve been working on it for a while now. I’ve been working on it for a while now, and I’ve been working on it for a while now. I’ve been working on it for a while now, and I’ve been working on',\n",
              " 'Hello everyone, I’m so excited to announce that I’m going to be writing this blog post. I’ve been working on this blog for a while now, and I’m excited to share it with you.\\n\\nI’ve been working on this blog for a while now, and I’m excited to share it with you. I’ve been working on this blog for a while now, and I’ve been working on']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dxFKHJJlDL8w"
      },
      "source": [
        "#@title Utils\n",
        "\"\"\"Taken from: https://stackoverflow.com/questions/7267226/range-for-floats/67053708#67053708\"\"\"\n",
        "\n",
        "def frange(start, stop, step, n=None):\n",
        "    \"\"\"return a WYSIWYG series of float values that mimic range behavior\n",
        "    by excluding the end point and not printing extraneous digits beyond\n",
        "    the precision of the input numbers (controlled by n and automatically\n",
        "    detected based on the string representation of the numbers passed).\n",
        "\n",
        "    EXAMPLES\n",
        "    ========\n",
        "\n",
        "    non-WYSIWYS simple list-comprehension\n",
        "\n",
        "    >>> [.11 + i*.1 for i in range(3)]\n",
        "    [0.11, 0.21000000000000002, 0.31]\n",
        "\n",
        "    WYSIWYG result for increasing sequence\n",
        "\n",
        "    >>> list(frange(0.11, .33, .1))\n",
        "    [0.11, 0.21, 0.31]\n",
        "\n",
        "    and decreasing sequences\n",
        "\n",
        "    >>> list(frange(.345, .1, -.1))\n",
        "    [0.345, 0.245, 0.145]\n",
        "\n",
        "    To hit the end point for a sequence that is divisibe by\n",
        "    the step size, make the end point a little bigger by\n",
        "    adding half the step size:\n",
        "\n",
        "    >>> dx = .2\n",
        "    >>> list(frange(0, 1 + dx/2, dx))\n",
        "    [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "\n",
        "    \"\"\"\n",
        "    if step == 0:\n",
        "        raise ValueError('step must not be 0')\n",
        "    # how many decimal places are showing?\n",
        "    if n is None:\n",
        "        n = max([0 if '.' not in str(i) else len(str(i).split('.')[1])\n",
        "                for i in (start, stop, step)])\n",
        "    if step*(stop - start) > 0:  # a non-null incr/decr range\n",
        "        if step < 0:\n",
        "            for i in frange(-start, -stop, -step, n):\n",
        "                yield -i\n",
        "        else:\n",
        "            steps = round((stop - start)/step)\n",
        "            while round(step*steps + start, n) < stop:\n",
        "                steps += 1\n",
        "            for i in range(steps):\n",
        "                yield round(start + i*step, n)\n",
        "\n",
        "def collect_generation_data(input: str, \n",
        "                            model: GPTNeoGenerator,\n",
        "                            beams=list(range(10, 100, 50)),\n",
        "                            temps=list(frange(0.1, 1.5, 0.8)),\n",
        "                            topks=list(range(10, 100, 50)),\n",
        "                            topps=list(frange(0.1, 1, 0.3)),\n",
        "                            **kwargs) -> dict:\n",
        "    data = {\n",
        "        \"config\": locals(),\n",
        "        \"results\": {\n",
        "            \"greedy\": [],\n",
        "            \"beam\": [],\n",
        "            \"sample\": [],\n",
        "            \"top_k\": [],\n",
        "            \"top_p\": [],\n",
        "            \"top_pk\": [],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Collect samples decoded via Greedy Search\n",
        "    data[\"results\"][\"greedy\"] = model.greedy(input, **kwargs)\n",
        "    data[\"results\"][\"beam\"] = [\n",
        "        {\n",
        "            \"num_beams\": n, \n",
        "            \"results\": [\n",
        "                model.beam(\n",
        "                    input, \n",
        "                    num_beams=n, \n",
        "                    **kwargs)\n",
        "            ]\n",
        "         } for n in beams\n",
        "    ]\n",
        "        \n",
        "    # Collect samples decoded via Sampling\n",
        "    data[\"results\"][\"sample\"] = [\n",
        "        {\n",
        "            \"temperature\": t,\n",
        "            \"results\": model.sample(\n",
        "                input, \n",
        "                temperature=t, \n",
        "                **kwargs)\n",
        "        } for t in temps\n",
        "    ]\n",
        "    # Collect samples decoded via Top-k Sampling\n",
        "    data[\"results\"][\"top_k\"] = [\n",
        "        {\n",
        "            \"top_k\": k,\n",
        "            \"results\": [\n",
        "                {\n",
        "                    \"temperature\": t,\n",
        "                    \"results\": model.top_k(\n",
        "                        input, \n",
        "                        top_k=k, \n",
        "                        temperature=t, \n",
        "                        **kwargs)\n",
        "                } for t in temps\n",
        "            ]\n",
        "        } for k in topks\n",
        "    ]\n",
        "    # Collect samples decoded via Top-p Sampling\n",
        "    data[\"results\"][\"top_p\"] = [\n",
        "        {\n",
        "            \"top_p\": p,\n",
        "            \"results\": [\n",
        "                {\n",
        "                    \"temperature\": t,\n",
        "                    \"results\": model.top_p(\n",
        "                        input,\n",
        "                        top_p=p,\n",
        "                        temperature=t, \n",
        "                        **kwargs)\n",
        "                } for t in temps\n",
        "            ]\n",
        "        } for p in topps\n",
        "    ]\n",
        "    # Collect samples decoded via Top-p and Top-k Sampling\n",
        "    data[\"results\"][\"top_pk\"] = [\n",
        "        {\n",
        "            \"top_p\": p,\n",
        "            \"results\": [\n",
        "                {\n",
        "                    \"top_k\": k,\n",
        "                    \"results\": [\n",
        "                        {\n",
        "                            \"temperature\": t,\n",
        "                            \"results\": model.top_k(\n",
        "                                input,\n",
        "                                top_p=p,\n",
        "                                top_k=k,\n",
        "                                temperature=t, \n",
        "                                **kwargs)\n",
        "                        } for t in temps\n",
        "                    ]\n",
        "                } for k in topks\n",
        "            ]\n",
        "        } for p in topps\n",
        "    ]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def recurse_through_data(data: Union[dict, list, str], key: str) -> Union[List[str], str]:\n",
        "    \"\"\"Find elements in given data specified via a key.\"\"\"\n",
        "    if isinstance(data, dict) and key in data:\n",
        "        return recurse_through_data(data[key], key)\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        return [recurse_through_data(element, key) for element in data]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def flatten(l: list) -> list:\n",
        "    return list(np.array(l).flat)\n",
        "\n",
        "\n",
        "def process_data(data: dict, key: str) -> list:\n",
        "    \"\"\"Filter data by key and return results as list.\"\"\"\n",
        "    data = recurse_through_data(data, key)\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        return flatten(data)\n",
        "    \n",
        "    return [data]\n",
        "\n",
        "\n",
        "def repeat_in_intervall(l: list, num_repeat: int, invervall: int) -> list:\n",
        "    \"\"\"Repeat the elements of a list n times and do so every intervall.\"\"\"\n",
        "    return [i for _ in range(invervall) for i in l for _ in range(num_repeat)]\n",
        "\n",
        "\n",
        "def export_data(data: dict):\n",
        "    config = data[\"config\"]\n",
        "    results = data[\"results\"]\n",
        "\n",
        "    beams = config[\"beams\"]\n",
        "    temps = config[\"temps\"]\n",
        "    topks = config[\"topks\"]\n",
        "    topps = config[\"topps\"]\n",
        "\n",
        "    nseqs = config[\"kwargs\"][\"num_return_sequences\"]\n",
        "    nbeam = len(beams)\n",
        "    ntemp = len(temps)\n",
        "    ntopk = len(topks)\n",
        "    ntopp = len(topps)\n",
        "\n",
        "    export = {}\n",
        "\n",
        "    export[\"Greedy Search\"] = results[\"greedy\"]\n",
        "\n",
        "    export[\"Number Beams\"] = repeat_in_intervall(beams, nseqs, 1)\n",
        "    export[\"Beam Search\"] = process_data(results[\"beam\"], \"results\")\n",
        "\n",
        "    export[\"Temperature Sample\"] = repeat_in_intervall(temps, nseqs, 1)\n",
        "    export[\"Sample\"] = process_data(results[\"sample\"], \"results\")\n",
        "\n",
        "    export[\"Number Top-k\"] = repeat_in_intervall(topks, nseqs*ntemp, 1)\n",
        "    export[\"Temperature Top-k\"] = repeat_in_intervall(temps, nseqs, ntopk)\n",
        "    export[\"Top-k\"] = process_data(results[\"top_k\"], \"results\")\n",
        "\n",
        "    export[\"Number Top-p\"] = repeat_in_intervall(topps, nseqs*ntemp, 1)\n",
        "    export[\"Temperature Top-p\"] = repeat_in_intervall(temps, nseqs, ntopp)\n",
        "    export[\"Top-p\"] = process_data(results[\"top_p\"], \"results\")\n",
        "\n",
        "    export[\"Top-p & Top-k Number Top-p\"] = repeat_in_intervall(topps, nseqs*ntemp*ntopk, 1)\n",
        "    export[\"Top-p & Top-k Number Top-k\"] = repeat_in_intervall(topks, nseqs*ntemp, ntopp)\n",
        "    export[\"Temperature Top-p & Top-k\"] = repeat_in_intervall(temps, nseqs, ntopk*ntopp)\n",
        "    export[\"Top-p & Top-k\"] = process_data(results[\"top_pk\"], \"results\")\n",
        "\n",
        "    for column in export:\n",
        "        export[column] = export[column] + [''] * (ntopp*nseqs*ntemp*ntopk - len(export[column]))\n",
        "\n",
        "    return export"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAEhn9jJH9fm"
      },
      "source": [
        "config = {\n",
        "    'max_length': 25,\n",
        "    'num_return_sequences': 2,\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlCrn1r9HzvU",
        "outputId": "5e1f2c80-1a3f-413a-9ea4-46ba5a5c5e6c"
      },
      "source": [
        "data = collect_generation_data(\"Hi\", gpt_neo_125m, **config)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Z3tNivRgAi"
      },
      "source": [
        "export = export_data(data)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t6uwILWAKty"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "\n",
        "df = pd.DataFrame(export)\n",
        "\n",
        "model = data[\"config\"][\"model\"].name\n",
        "model = model.replace(\"/\", \"_\")\n",
        "\n",
        "time = datetime.datetime.now()\n",
        "time = f\"{now:%Y%m%d%H%M}\"\n",
        "\n",
        "df.to_excel(f\"GenerationResearch_{model}_{time}.xlsx\", )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}